\documentclass[twocolumn]{article}

\title{Visualising Courtship Index from Videos of Drosophila}

\author{Michael Dewar, Tim Lukins and Douglas Armstrong}


%\category{H5.m}{Information interfaces and presentation}{Misc}

\begin{document}
	
	\maketitle

	\begin{abstract}
	We present approach to the automated analysis and visualisation of courtship index (CI) in \emph{Drosophila} - usually defined as the percentage of time the male fruit fly spends displaying courtship behaviour over a fixed period. Using machine vision techniques we extract position and orientation features from video of courting flies. These features are then used in a classifier in order to determine the presence of courtship behaviour in each frame. Using this information we generate a set of visualisations which emphasises the time course of the behaviour. Hence, we arrive at a much richer interpretation of courtship index, one that highlights the dynamic aspects of \emph{Drosophila} courtship behaviour, exposing phenotypic information that the standard, manual assay would typically miss.
	\end{abstract}

\section{Introduction}

Courtship index (CI) in \emph{Drosophila} is defined as the percentage of time spent by the male exhibiting any kind of courtship behaviour in a given period \cite{}. It is typically collected manually, by watching videos of courting flies and starting and stopping a stopwatch when courtship behaviour is observed. CI is a useful summary statistic, used to quantify a phenotypic change, and to correlate against a change in neurological structure \cite{}.

A trained individual is capable of recording courtship in four flies at a time, or in one fly at up to 4$\times$ real-time video playback. This method of recording courtship index involves two gross forms of information attenuation. The first is the attenuation of phenotypic information - compressing a large number of different behaviours into a single ``courtship'' behaviour (such as singing, chasing, dancing etc). The second is the attenuation of temporal information - compressing the time course of courtship behaviour into a single percentage value. This paper aims to address the second of these issues, using automatic machine vision and machine learning techniques to overcome the inherent limitations of manual analysis that necessitates temporal attenuation.

The use of automated analysis to overcome this attenuation has a set of additional benefits. The most striking of these in practice is the reduction of labour - the methods reported below work faster than real time, removing the need to perform routine viewing of the videos and allowing the biologist to focus on novel phenotypes that arise. Another benefit is consistency - these methods always return the same results when presented with the same data. This allows comparisons across studies without dealing with (or guiltily ignoring) the problem that different individuals can analyse the same video quite differently. The final benefit of these methods is the ability to publish the classifier used to generate the results along with the collected video and feature data.

Our approach has two main components: tracking and classification. The tracking component combines a background model, a sequence of image manipulations and a basic model of fly movement in order to extract position and orientation features from the video. The classification component uses a labelled set of videos along with their extracted features to train a decision tree, which can be used to classify any subsequent videos. The remainder of the paper describes these two components, as well as a visualisation of the output of the decision tree in order to provide a rich summary of the courtship behaviour. A final classifier is used to separate markedly different courtship phenotypes based on the information extracted from the videos.

\section{Tracking}

The video data consists of (at least one set of) a pair of courting flies. In order to decide whether or not courtship is taking place, we need to extract the position and orientation of each fly in the video. 

The video itself is envisaged to come from a standard experimental setup: a single camera, viewing the courtship chamber(s) from above, with illumination coming from a set of non-specialist lights. The camera will likely be a hand-held camcorder or an IP camera (webcam) operating at around 30 frames per second. 

This presents the first problem for the tracking approach - such videos are considered to be of a low quality. The fly takes up as little as a 10$\times$5 rectangle of pixels; the frame rate is low compared to some of the faster movements performed by the fly; lighting, reflections and shadows clutter the scene; video compression artefacts distort the sequence of images. 

The second problem for the tracking approach is the movement of the flies. A smoothly moving object in a scene is relatively straight forward to track as its movements are quite predictable. A courting Drosophila, on the other hand, exhibits highly non-smooth, unpredictable movement. At best we can say that the flies mostly move forward. 

The developed tracking algorithm attempts to deal with these issues in a straightforward and robust manner. We first fit ellipsoids to the flies before trying to determine their orientation. The orientation is determined using a smoothness constraint: it is unlikely that the fly will turn through a large angle in a single frame transition, and the observation that flies are much more likely to move in the direction they are facing.

Our algorithm is summarised in Table \ref{tab:tracking}, and illustrated in Figure \ref{fig:tracking}. 

\begin{table}
\begin{enumerate}
	\item create background model
	\item for each frame:
	\begin{enumerate}
		\item remove codec artefacts
		\item remove background model
		\item threshold
		\item mask
		\item find contours
		\item fit ellipses
		\item filter inappropriate ellipses
		\item choose the two ellipses closest to the previous ellipses
		\item choose orientation based on movement
	\end{enumerate}
\end{enumerate}
\label{tab:tracking}
\caption{Each step of the tracking algorithm used for extracting the position and orientation from a low-quality video.}
\end{table}

\begin{figure}
	
	\caption{Illustration of each step of the algorithm. Shown is (a) the background model, (b) the thresholded image, (c) the extracted contours, (d) the resulting ellipses.}
	\label{fig:tracking}
\end{figure}

\section{Classification}

In order to extract standard CI from a period video we need to be able to determine how long the fly spends courting in this period. When looking at all the various aspects of courtship behaviour we are able to highlight two features that are sufficient to characterise the vast majority of them: proximity and of male orientation with respect to the female (`pointing'). 

For example, the courtship dance displayed by the male is typically performed within a certain distance, with the male maintaining his orientation toward the female at all times. Chasing, tapping and licking are all performed at very close proximity with male orientation toward the female. Due to the sensing abilities of the male fly, courtship rarely takes place at a large distance, and if the male is close to and facing the female for much longer than a second it is unlikely that anything but courtship is taking place.

However, this relationship between proximity, pointing and courtship is not a straight-forward one, and can change depending on the experimental setup. In addition, the details associated with how close a male fly should be to the female in order to be considered courting, or how much he should be facing her, are not universally agreed on. 

Hence a technique that can learn the non-linear relationship between proximity, pointing and courtship, that can be adapted to a user's idea of what is considered courtship, is required. To deliver this we present a decision tree technique that is able to learn this relationship and hence classify each frame of a video as courting or not courting. 

\section{Visualisation}

Different visualisation options

\section{Classifying Courtship Dynamics}

Parmeterisation of the courtship index curves

\section{Performance}

Performance of the algorithm

\section{Conclusion}
Conclusion

\end{document}